
He pensat crear aquest document per anar afegint comentaris que veiem que poden ser interessants per a que l'altre pugui continuar investigant.

22/03/2019 - Carlos
      Line 24
      En aquest bucle recupero el títol. Enlloc de buscar-lo per l'etiqueta <h1>, mirant el codi es podría buscar per l'etiqueta <div> i    class="title", i recuperar el valor del fill, que sería el tag <h1>
      Hi hauria una forma de recuperar el títol sense fer servir un bucle? No he aconseguit recuperar el valor sense un bucle, anant al tag <div> i class= 'title'. Podría ser una millor forma de recuperar el valor, i potser més aconsellable que no anar al tag <h1>0
	  
	  Line 26
      En aquest bucle vaig a l'estructura de preus, per recuperar els preus i el descompte. Igual que en el cas anterior, sería necessari un bucle for? Si no es repeteix l'etiqueta no sería necessari, però no ho he aconseguit.

      Line 35-36
      Executant els dos enllaços d'aquestes dues línies, pel primer enllaç
	  
	 
22/03/2019 - Oscar

	Títol sense bucle -->  title = soup.find('div',{'class':'title'}).h1
	
	capturada platform --> platform = soup.find('div',{'class':'subinfos'}).a.get('class')
	capturada releaseDate
	
	vigilar amb l'estructura prices. No sempre existeixen els 3 valors (retail, price, discount). Sobretot en la part dels DLCs.
	
	exemple:
	https://www.instant-gaming.com/es/186-comprar-key-rockstar-grand-theft-auto-v/
	 
	
	Jo l'estructura de prices la faria sense bucle:
	
	item_prices = soup.find('div', {'class':'prices'})      
    retail = item_prices.find('span')
    price = item_prices.find('div',{'class':'price'})
    discount = item_prices.find('div',{'class':'discount'})
    print(retail.string)
    print(price.string)
    print(discount.string)
	
	els adjunts (DLCs) ja sortiran quan s'accedeixi a la seva pàgina.
	
23/03/2019 - Oscar
	afegit category i users_rating

23/03/2019 - Carlos
	He modificat el codi per deixar l'estructura de prices sense el bucle, ja que la solució que has donat m'ha semblat bona.
	Per recuperar la plataforma, he modificat la manera de recuperar-la ja que, per exemple, per Uplay, retornaba "platform uplay". Un cop modificada la fórmula retorna únicament el nom de la plataforma.
	
	Pel cas de retail i discount, he afegit un try..except..., per a que no li asigni cap variable en cas que no existeixi el camp per a un joc en concret.
	
	Finalment, he anat probant per a guardar els resultats en un arxiu csv. He començat per guardar únicament la plataforma  i el títol dels jocs. He creat una variable data, on vaig guardant els valors que vull guardar al csv. Despres, per a cada cop que executa la funció get_single_item_data, escric a l'arxiu csv.
	
	Més cap al final, he ficat els dos mètodes que he utilitzat per a probar a guardar a l'arxiu csv. El primer amb tota la primera pagina, i el segon amb 3 jocs únicament.
	El primer pas es obrir l'arxiu on guardarem les dades. Posteriorment creem la variable writer, i desprès executem les funcions que volem, o el trade_spider o el get_single_item_data.
	
	Afegeixo l'arxiu que he generat. No hi ha molts resultats ja que hem donaba un error amb un joc, y nomès ha escrit 3 jocs. He de continuar mirant aquest error que hem dona. Es un error de UnicodeEncodeError. Hem genera també una línea buida entre cada joc, he de mirar perquè. Ja hem dius què opines sobre la manera d'anar guardant a l'arxiu. He probat tambè d'anar guardant els resultats a una llista de llistes, y desprès al final guardar tota la llista a l'arxiu, però m'ho guardaba tot a diferents columnes, no files. Hem falta afegir tambè la capçalera a l'arxiu, que crec que ja sé com fer-ho.
	
	Qualsevol suggerencia me dius, o hem truques i ho anem parlant.
	
	
23/03/2019 - Oscar

He modificat la get_single_item_data pq retorni un vector i concatenar-ho amb la categoria.
Els vectors retornats per cada pàgina es guarden en un dataframe, la manera que he trobat no és la més elegant, així que si creus que hi ha manera millors endavant. Al final es pasa el dataframe sencer a csv.
Falta posar els noms de les variables en el dataframe.
T'adjunto el data.csv generat.

29/03/2019 - Carlos
Ja he afegit una columna amb la data d'extracció.
He modificat la columna de category, per a que mostri GAME, DLC o UNKNOWN, en cas que no sigui cap dels dos valors.
He creat una funció que busca el nombre total de pàgines.
He esborrat tots els print que hi havia al codi i no eren necessaris, i he afegit uns prints per anar mostrant en quina pàgina està, i saber quan pot trigar a acabar el scraper.
He modificat el nom del fitxer que es genera per a que es mostri la data i l'hora de l'extracció.
Adjunto el nou fitxer amb les dades de les 17 pàgines.
Qualsevol cosa me dius. Demà revisaré el codi per veure que no falti res.


30/03/2019 - Oscar

Acabo de pujar el primer esborrany de la part teòrica de la pràctica.
El diumenge o dilluns el tornarè a repassar per intentar millorar alguns punts. Si fas algun canvi, marca-ho amb un altre color.
Sobre el dataset està genial, pero crec que hauriem de treure les unitats (€, percentatge etc) i posar el mateix format de dates.
Qualsevol cosa hem dius.

30/03/2019 - Carlos
He afegit dos arxius .py. L'arxiu main.py simplement fa la trucada a les funcions de l'arxiu scraper.py.
He afegit en una funció apart (df_to_csv()) la creació de l'arxiu csv.
He creat una funció a scraper.py, on creo un diccionari amb els mesos, per a convertir el mes actual a castellà, i mantenir el mateix format de dates.
He el·liminat les unitats (€ i %), i ho he afegit a la capçalera, per a que es sàpigui en quines unitats estàn els camps.
Al descompte, he tret tambè el signe negatiu, ja que crec que, al indicar que la col·lumna es descompte, ja es suposa que s'haurà de restar.
Adjunto el nou arxiu generat.
Demà li dono una ullada al document teòric de la pràctica.
Qualsevol cosa hem dius.

31/03/2019 - Carlos
Me llegit el document word, i la veritat que el trobo molt complert, no trobo gaire cosa que es pugui millorar. Continuaré pensant algun punt per millorar, pero ho has deixat molt complert. He afegit únicament una col·lumna més al punt 5, indicant la data d'extracció.
Pel que fa a la llicència, buscant per internet, al final he afegit un arxiu LICENSE.txt amb la llicència que aplica. He anat a la pàgina de creative commons, he buscat la llicència que apliquem, i ja hem donen allà un text, que he aplicat a l'arxiu creat, per a indicar la llicència que aplica. Per una altra part, més endavant, hauríem de fer el repositori públic, ja que ara per ara es privat i nomès podem accedir nosaltres dos. Si volem que el professor pugui accedir haurem de fer-lo públic.
Per una altra part, he creat una carpeta "Datasets", per anar afegint allà els diferents arxius dels diferents dies. Ara per ara hi ha el del día 30 i el del dia 31. També he creat diferents carpetes per ordenar els arxius.
Trobo que estaría bé crear un arxiu README.txt, indicant qué hi ha a cada carpeta, i com executar el codi, és a dir, executar l'arxiu main.py.
Anem parlant.
